{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Summarization and Analysis Notebook\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load processed logs from Glue/S3\n",
    "2. Generate human-readable summaries\n",
    "3. Identify critical errors\n",
    "4. Suggest potential fixes\n",
    "\n",
    "It builds on the existing log processing infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# For embedding generation\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# For vector storage\n",
    "import faiss\n",
    "\n",
    "# For clustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# For LLM-based summarization\n",
    "import openai\n",
    "\n",
    "# Configure AWS services\n",
    "s3 = boto3.client('s3')\n",
    "athena = boto3.client('athena')\n",
    "glue = boto3.client('glue')\n",
    "\n",
    "# Set your OpenAI API key\n",
    "# openai.api_key = \"your-api-key\"  # Uncomment and set your API key\n",
    "\n",
    "# S3 bucket and paths\n",
    "LOG_BUCKET = \"first-order-application-logs\"\n",
    "ATHENA_RESULTS = f\"s3://{LOG_BUCKET}/athena-results/\"\n",
    "MODEL_PATH = f\"s3://{LOG_BUCKET}/models/\"\n",
    "\n",
    "# Glue database and table\n",
    "GLUE_DATABASE = \"first-order-glue-db\"\n",
    "LOGS_TABLE = \"fluent_bit_logs\"  # This should match the table created by your Glue crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Log Data\n",
    "\n",
    "First, we'll query Athena to get the processed logs from the Glue database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def run_athena_query(query, database=GLUE_DATABASE, output_location=ATHENA_RESULTS):\n",
    "    \"\"\"Run a query on Athena and return the results as a DataFrame\"\"\"\n",
    "    response = athena.start_query_execution(\n",
    "        QueryString=query,\n",
    "        QueryExecutionContext={\n",
    "            'Database': database\n",
    "        },\n",
    "        ResultConfiguration={\n",
    "            'OutputLocation': output_location,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    query_execution_id = response['QueryExecutionId']\n",
    "    \n",
    "    # Wait for query to complete\n",
    "    state = 'RUNNING'\n",
    "    while state in ['RUNNING', 'QUEUED']:\n",
    "        response = athena.get_query_execution(QueryExecutionId=query_execution_id)\n",
    "        state = response['QueryExecution']['Status']['State']\n",
    "        if state == 'FAILED':\n",
    "            raise Exception(f\"Query failed: {response['QueryExecution']['Status']['StateChangeReason']}\")\n",
    "        elif state == 'SUCCEEDED':\n",
    "            # Get the results\n",
    "            results = athena.get_query_results(QueryExecutionId=query_execution_id)\n",
    "            \n",
    "            # Parse the results into a DataFrame\n",
    "            columns = [col['Label'] for col in results['ResultSet']['ResultSetMetadata']['ColumnInfo']]\n",
    "            rows = []\n",
    "            for row in results['ResultSet']['Rows'][1:]:  # Skip the header row\n",
    "                data = [field.get('VarCharValue', '') if 'VarCharValue' in field else None for field in row['Data']]\n",
    "                rows.append(data)\n",
    "            \n",
    "            df = pd.DataFrame(rows, columns=columns)\n",
    "            return df\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Get the table schema\n",
    "def get_table_schema(database=GLUE_DATABASE, table=LOGS_TABLE):\n",
    "    \"\"\"Get the schema of a Glue table\"\"\"\n",
    "    response = glue.get_table(DatabaseName=database, Name=table)\n",
    "    return response['Table']['StorageDescriptor']['Columns']\n",
    "\n",
    "# Try to get the table schema\n",
    "try:\n",
    "    schema = get_table_schema()\n",
    "    print(f\"Table schema: {schema}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error getting table schema: {e}\")\n",
    "    print(\"This might be because the Glue crawler hasn't run yet or the table doesn't exist.\")\n",
    "    print(\"We'll continue with a sample query that should work with most log formats.\")\n",
    "    schema = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Query logs from the last 24 hours\n",
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM {LOGS_TABLE}\n",
    "WHERE date >= TIMESTAMP '{(datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d %H:%M:%S')}'\n",
    "LIMIT 1000\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    logs_df = run_athena_query(query)\n",
    "    print(f\"Retrieved {len(logs_df)} logs\")\n",
    "    display(logs_df.head())\n",
    "except Exception as e:\n",
    "    print(f\"Error querying logs: {e}\")\n",
    "    print(\"Using sample data instead...\")\n",
    "    \n",
    "    # Create sample data for demonstration\n",
    "    logs_df = pd.DataFrame({\n",
    "        'date': [datetime.now() - timedelta(minutes=i) for i in range(10)],\n",
    "        'log': [\n",
    "            \"[2025/04/01 23:59:58] [ info] [output:s3:s3.0] Successfully uploaded object /path/to/object\",\n",
    "            \"[2025/04/01 23:59:57] [ info] [input:tail:tail.0] Scanning log file /var/log/containers/app-xyz.log\",\n",
    "            \"[2025/04/01 23:59:56] [ error] [output:s3:s3.0] Failed to upload object: AccessDenied\",\n",
    "            \"[2025/04/01 23:59:55] [ warn] [filter:kubernetes:kubernetes.0] Missing annotation\",\n",
    "            \"[2025/04/01 23:59:54] [ error] [output:s3:s3.0] Connection timeout after 30s\",\n",
    "            \"[2025/04/01 23:59:53] [ info] [input:tail:tail.0] File rotated: /var/log/containers/app-xyz.log\",\n",
    "            \"[2025/04/01 23:59:52] [ error] [parser:json:json.0] Invalid JSON format\",\n",
    "            \"[2025/04/01 23:59:51] [ warn] [output:s3:s3.0] Retrying upload (attempt 3/5)\",\n",
    "            \"[2025/04/01 23:59:50] [ info] [engine] Shutting down\",\n",
    "            \"[2025/04/01 23:59:49] [ info] [engine] Started (version 1.8.15)\"\n",
    "        ],\n",
    "        'kubernetes.pod_name': ['app-xyz-1234', 'app-xyz-1234', 'app-xyz-1234', 'app-xyz-1234', 'app-xyz-1234',\n",
    "                              'app-xyz-1234', 'app-xyz-1234', 'app-xyz-1234', 'fluent-bit-qmn54', 'fluent-bit-qmn54'],\n",
    "        'kubernetes.namespace_name': ['default', 'default', 'default', 'default', 'default',\n",
    "                                    'default', 'default', 'default', 'logging', 'logging']\n",
    "    })\n",
    "    display(logs_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Features and Preprocess Logs\n",
    "\n",
    "Now we'll extract features from the logs, similar to what your existing log processor does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def extract_features(logs_df):\n",
    "    \"\"\"Extract features from logs DataFrame\"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df = logs_df.copy()\n",
    "    \n",
    "    # Initialize new columns\n",
    "    df['log_level'] = None\n",
    "    df['component'] = None\n",
    "    df['action'] = None\n",
    "    df['status'] = None\n",
    "    df['error_type'] = None\n",
    "    df['error_details'] = None\n",
    "    \n",
    "    # Extract features from log messages\n",
    "    for idx, row in df.iterrows():\n",
    "        log_msg = row.get('log', '')\n",
    "        \n",
    "        # Extract log level\n",
    "        level_match = re.search(r'\\[(info|warn|error|debug)\\]', log_msg, re.IGNORECASE)\n",
    "        if level_match:\n",
    "            df.at[idx, 'log_level'] = level_match.group(1).lower()\n",
    "        \n",
    "        # Extract component\n",
    "        component_match = re.search(r'\\[([^:]+):([^:]+):([^\\]]+)\\]', log_msg)\n",
    "        if component_match:\n",
    "            df.at[idx, 'component'] = f\"{component_match.group(1)}:{component_match.group(2)}:{component_match.group(3)}\"\n",
    "        \n",
    "        # Extract action and status\n",
    "        if \"Successfully\" in log_msg and \"uploaded\" in log_msg:\n",
    "            df.at[idx, 'action'] = \"upload\"\n",
    "            df.at[idx, 'status'] = \"success\"\n",
    "        elif \"Failed to upload\" in log_msg:\n",
    "            df.at[idx, 'action'] = \"upload\"\n",
    "            df.at[idx, 'status'] = \"failure\"\n",
    "            \n",
    "            # Extract error details\n",
    "            error_match = re.search(r'Failed to upload.*: (\\w+)', log_msg)\n",
    "            if error_match:\n",
    "                df.at[idx, 'error_type'] = error_match.group(1)\n",
    "                df.at[idx, 'error_details'] = log_msg.split(\"Failed to upload\")[1].strip()\n",
    "        elif \"Connection timeout\" in log_msg:\n",
    "            df.at[idx, 'action'] = \"connect\"\n",
    "            df.at[idx, 'status'] = \"failure\"\n",
    "            df.at[idx, 'error_type'] = \"Timeout\"\n",
    "            \n",
    "            # Extract timeout details\n",
    "            timeout_match = re.search(r'timeout after (\\d+)s', log_msg)\n",
    "            if timeout_match:\n",
    "                df.at[idx, 'error_details'] = f\"Timeout after {timeout_match.group(1)} seconds\"\n",
    "        elif \"Invalid JSON\" in log_msg:\n",
    "            df.at[idx, 'action'] = \"parse\"\n",
    "            df.at[idx, 'status'] = \"failure\"\n",
    "            df.at[idx, 'error_type'] = \"InvalidFormat\"\n",
    "            df.at[idx, 'error_details'] = \"Invalid JSON format\"\n",
    "        elif \"Scanning log file\" in log_msg:\n",
    "            df.at[idx, 'action'] = \"scan\"\n",
    "            df.at[idx, 'status'] = \"info\"\n",
    "        elif \"File rotated\" in log_msg:\n",
    "            df.at[idx, 'action'] = \"rotate\"\n",
    "            df.at[idx, 'status'] = \"info\"\n",
    "        elif \"Retrying\" in log_msg and \"upload\" in log_msg:\n",
    "            df.at[idx, 'action'] = \"retry\"\n",
    "            df.at[idx, 'status'] = \"warning\"\n",
    "            \n",
    "            # Extract retry details\n",
    "            retry_match = re.search(r'attempt (\\d+)/(\\d+)', log_msg)\n",
    "            if retry_match:\n",
    "                df.at[idx, 'error_details'] = f\"Attempt {retry_match.group(1)} of {retry_match.group(2)}\"\n",
    "        elif \"Started\" in log_msg:\n",
    "            df.at[idx, 'action'] = \"start\"\n",
    "            df.at[idx, 'status'] = \"info\"\n",
    "        elif \"Shutting down\" in log_msg:\n",
    "            df.at[idx, 'action'] = \"shutdown\"\n",
    "            df.at[idx, 'status'] = \"info\"\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Extract features\n",
    "features_df = extract_features(logs_df)\n",
    "display(features_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Embeddings and Cluster Logs\n",
    "\n",
    "We'll use embeddings to cluster similar logs together, which will help in summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def generate_embeddings(df, text_column='log', model_name=\"all-MiniLM-L6-v2\"):\n",
    "    \"\"\"Generate embeddings for log messages\"\"\"\n",
    "    # Load the model\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    texts = df[text_column].fillna('').tolist()\n",
    "    embeddings = model.encode(texts, show_progress_bar=True)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "def cluster_logs(embeddings, eps=0.5, min_samples=2):\n",
    "    \"\"\"Cluster logs using DBSCAN\"\"\"\n",
    "    # Normalize embeddings\n",
    "    normalized_embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    \n",
    "    # Cluster using DBSCAN\n",
    "    clustering = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine').fit(normalized_embeddings)\n",
    "    \n",
    "    return clustering.labels_\n",
    "\n",
    "# Generate embeddings\n",
    "try:\n",
    "    embeddings = generate_embeddings(features_df)\n",
    "    print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "    \n",
    "    # Cluster logs\n",
    "    cluster_labels = cluster_logs(embeddings)\n",
    "    features_df['cluster'] = cluster_labels\n",
    "    \n",
    "    # Display cluster statistics\n",
    "    cluster_counts = features_df['cluster'].value_counts()\n",
    "    print(f\"Number of clusters: {len(cluster_counts)}\")\n",
    "    print(f\"Noise points (cluster -1): {cluster_counts.get(-1, 0)}\")\n",
    "    display(cluster_counts.head())\n",
    "except Exception as e:\n",
    "    print(f\"Error generating embeddings or clustering: {e}\")\n",
    "    print(\"This might be because the sentence-transformers package is not installed.\")\n",
    "    print(\"You can install it with: pip install sentence-transformers\")\n",
    "    \n",
    "    # Assign random clusters for demonstration\n",
    "    import random\n",
    "    features_df['cluster'] = [random.randint(0, 3) for _ in range(len(features_df))]\n",
    "    print(\"Assigned random clusters for demonstration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Identify Critical Errors and Patterns\n",
    "\n",
    "Now we'll identify critical errors and patterns in the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def identify_critical_errors(df):\n",
    "    \"\"\"Identify critical errors in logs\"\"\"\n",
    "    # Define critical error patterns\n",
    "    critical_patterns = [\n",
    "        # Access denied errors\n",
    "        {'pattern': 'AccessDenied', 'severity': 'high', 'category': 'permission', \n",
    "         'description': 'S3 access denied error', 'column': 'error_type'},\n",
    "        # Connection timeouts\n",
    "        {'pattern': 'Timeout', 'severity': 'medium', 'category': 'network', \n",
    "         'description': 'Connection timeout', 'column': 'error_type'},\n",
    "        # Invalid formats\n",
    "        {'pattern': 'InvalidFormat', 'severity': 'medium', 'category': 'data', \n",
    "         'description': 'Invalid data format', 'column': 'error_type'},\n",
    "        # Retry attempts\n",
    "        {'pattern': 'retry', 'severity': 'low', 'category': 'operation', \n",
    "         'description': 'Operation retry', 'column': 'action'}\n",
    "    ]\n",
    "    \n",
    "    # Find matches\n",
    "    critical_errors = []\n",
    "    \n",
    "    for pattern in critical_patterns:\n",
    "        column = pattern['column']\n",
    "        matches = df[df[column] == pattern['pattern']]\n",
    "        \n",
    "        if len(matches) > 0:\n",
    "            for _, row in matches.iterrows():\n",
    "                error = {\n",
    "                    'timestamp': row.get('date'),\n",
    "                    'log': row.get('log'),\n",
    "                    'pod_name': row.get('kubernetes.pod_name'),\n",
    "                    'namespace': row.get('kubernetes.namespace_name'),\n",
    "                    'severity': pattern['severity'],\n",
    "                    'category': pattern['category'],\n",
    "                    'description': pattern['description'],\n",
    "                    'details': row.get('error_details'),\n",
    "                    'count': 1\n",
    "                }\n",
    "                critical_errors.append(error)\n",
    "    \n",
    "    # Group similar errors\n",
    "    grouped_errors = {}\n",
    "    for error in critical_errors:\n",
    "        key = f\"{error['severity']}_{error['category']}_{error['description']}\"\n",
    "        if key in grouped_errors:\n",
    "            grouped_errors[key]['count'] += 1\n",
    "            # Keep the most recent occurrence\n",
    "            if error['timestamp'] > grouped_errors[key]['timestamp']:\n",
    "                grouped_errors[key]['timestamp'] = error['timestamp']\n",
    "                grouped_errors[key]['log'] = error['log']\n",
    "                grouped_errors[key]['details'] = error['details']\n",
    "        else:\n",
    "            grouped_errors[key] = error\n",
    "    \n",
    "    return list(grouped_errors.values())\n",
    "\n",
    "def identify_patterns(df):\n",
    "    \"\"\"Identify patterns in logs\"\"\"\n",
    "    patterns = {}\n",
    "    \n",
    "    # Group by cluster\n",
    "    for cluster_id, cluster_df in df[df['cluster'] != -1].groupby('cluster'):\n",
    "        # Skip small clusters\n",
    "        if len(cluster_df) < 2:\n",
    "            continue\n",
    "            \n",
    "        # Get the most common component and action\n",
    "        component = cluster_df['component'].mode().iloc[0] if not cluster_df['component'].isna().all() else 'unknown'\n",
    "        action = cluster_df['action'].mode().iloc[0] if not cluster_df['action'].isna().all() else 'unknown'\n",
    "        status = cluster_df['status'].mode().iloc[0] if not cluster_df['status'].isna().all() else 'unknown'\n",
    "        \n",
    "        # Calculate time intervals\n",
    "        if 'date' in cluster_df.columns:\n",
    "            try:\n",
    "                # Convert to datetime if it's not already\n",
    "                if not pd.api.types.is_datetime64_any_dtype(cluster_df['date']):\n",
    "                    cluster_df['date'] = pd.to_datetime(cluster_df['date'])\n",
    "                    \n",
    "                # Sort and calculate intervals\n",
    "                sorted_df = cluster_df.sort_values('date')\n",
    "                sorted_df['time_diff'] = sorted_df['date'].diff().dt.total_seconds()\n",
    "                \n",
    "                median_interval = sorted_df['time_diff'].median()\n",
    "                mean_interval = sorted_df['time_diff'].mean()\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating time intervals: {e}\")\n",
    "                median_interval = None\n",
    "                mean_interval = None\n",
    "        else:\n",
    "            median_interval = None\n",
    "            mean_interval = None\n",
    "        \n",
    "        # Create pattern entry\n",
    "        pattern_key = f\"cluster_{cluster_id}\"\n",
    "        patterns[pattern_key] = {\n",
    "            'cluster_id': int(cluster_id),\n",
    "            'component': component,\n",
    "            'action': action,\n",
    "            'status': status,\n",
    "            'count': len(cluster_df),\n",
    "            'median_interval_seconds': float(median_interval) if pd.notna(median_interval) else None,\n",
    "            'mean_interval_seconds': float(mean_interval) if pd.notna(mean_interval) else None,\n",
    "            'sample_logs': cluster_df.head(3)['log'].tolist()\n",
    "        }\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "# Identify critical errors\n",
    "critical_errors = identify_critical_errors(features_df)\n",
    "print(f\"Identified {len(critical_errors)} critical errors\")\n",
    "for error in critical_errors:\n",
    "    print(f\"- {error['severity'].upper()} {error['category']}: {error['description']} (count: {error['count']})\")\n",
    "    print(f\"  Example: {error['log']}\")\n",
    "    print()\n",
    "\n",
    "# Identify patterns\n",
    "patterns = identify_patterns(features_df)\n",
    "print(f\"\\nIdentified {len(patterns)} patterns\")\n",
    "for key, pattern in list(patterns.items())[:3]:  # Show first 3 patterns\n",
    "    print(f\"- Cluster {pattern['cluster_id']}: {pattern['component']} {pattern['action']} ({pattern['count']} logs)\")\n",
    "    print(f\"  Example: {pattern['sample_logs'][0] if pattern['sample_logs'] else 'No sample'}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Human-Readable Summaries and Suggest Fixes\n",
    "\n",
    "Now we'll use an LLM to generate human-readable summaries and suggest fixes for the critical errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example fixes for common errors\n",
    "common_fixes = {\n",
    "    'AccessDenied': [\n",
    "        \"Check IAM permissions for the FluentBit service account\",\n",
    "        \"Verify S3 bucket policy allows write access from the cluster's IP range\",\n",
    "        \"Ensure KMS key permissions are properly configured if using SSE-KMS\"\n",
    "    ],\n",
    "    'Timeout': [\n",
    "        \"Check network connectivity between the cluster and S3 endpoint\",\n",
    "        \"Verify VPC endpoints are properly configured\",\n",
    "        \"Consider increasing the timeout setting in FluentBit configuration\"\n",
    "    ],\n",
    "    'InvalidFormat': [\n",
    "        \"Review the log format configuration in FluentBit\",\n",
    "        \"Check for malformed JSON in application logs\",\n",
    "        \"Add a parser filter to handle the specific log format\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Display suggested fixes for identified errors\n",
    "print(\"Suggested Fixes for Critical Errors:\\n\")\n",
    "for error in critical_errors:\n",
    "    if error['severity'] == 'low':\n",
    "        continue\n",
    "        \n",
    "    print(f\"For {error['severity'].upper()} {error['category']}: {error['description']}\")\n",
    "    \n",
    "    # Get fixes based on error type\n",
    "    if error['error_type'] in common_fixes:\n",
    "        for i, fix in enumerate(common_fixes[error['error_type']], 1):\n",
    "            print(f\"  {i}. {fix}\")\n",
    "    else:\n",
    "        print(\"  No specific fixes available for this error type.\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate a comprehensive summary report\n",
    "summary_text = f\"# Log Analysis Summary Report\\n\\n\"\n",
    "summary_text += f\"**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n",
    "summary_text += f\"**Total Logs Analyzed:** {len(features_df)}\\n\\n\"\n",
    "\n",
    "# Add critical errors section\n",
    "if critical_errors:\n",
    "    summary_text += f\"## Critical Errors ({len(critical_errors)})\\n\\n\"\n",
    "    for error in critical_errors:\n",
    "        summary_text += f\"### {error['severity'].upper()} {error['category']}: {error['description']}\\n\"\n",
    "        summary_text += f\"- **Count:** {error['count']}\\n\"\n",
    "        summary_text += f
