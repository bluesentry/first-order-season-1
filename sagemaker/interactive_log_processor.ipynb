{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Log Processing and Analysis\n",
    "\n",
    "This notebook allows you to interactively test the log processing solution without deploying the Glue job. You can run each cell step by step, see the results, and modify the code as needed.\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. Launch this notebook in SageMaker Studio or a SageMaker notebook instance\n",
    "2. Make sure your IAM role has access to the S3 bucket containing your logs\n",
    "3. Run each cell sequentially to process and analyze your logs\n",
    "4. Modify the code as needed to customize the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "First, let's install the required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers faiss-cpu torch pandas numpy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import gzip\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from urllib.parse import urlparse\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# For embedding generation\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# For vector storage\n",
    "import faiss\n",
    "import pickle\n",
    "\n",
    "# Configure plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Initialize AWS clients\n",
    "s3 = boto3.client('s3')\n",
    "sns_client = boto3.client('sns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "Set your S3 bucket and paths here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 Configuration\n",
    "S3_BUCKET = \"first-order-application-logs\"  # Your S3 bucket name\n",
    "S3_INPUT_PATH = f\"s3://{S3_BUCKET}/fluent-bit-logs/\"  # Path to your log files\n",
    "S3_OUTPUT_PATH = f\"s3://{S3_BUCKET}/log-analysis/\"  # Path to save analysis results\n",
    "\n",
    "# Model Configuration\n",
    "MODEL_NAME = \"all-MiniLM-L6-v2\"  # Sentence transformer model\n",
    "VECTOR_DIM = 384  # Embedding dimension\n",
    "\n",
    "# Analysis Configuration\n",
    "MIN_GROUP_SIZE = 3  # Minimum number of logs to consider a pattern\n",
    "MAX_LOGS_TO_PROCESS = 1000  # Maximum number of logs to process (set to None for all logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Log Processor Class\n",
    "\n",
    "This is the main class for processing and analyzing logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractiveLogProcessor:\n",
    "    \"\"\"\n",
    "    A class to process, analyze, and summarize logs interactively.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model_name: str = \"all-MiniLM-L6-v2\", \n",
    "                 vector_dim: int = 384,\n",
    "                 s3_model_path: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize the log processor\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name of the sentence transformer model\n",
    "            vector_dim: Dimension of the embedding vectors\n",
    "            s3_model_path: Path to load model from S3\n",
    "        \"\"\"\n",
    "        # Initialize S3 client\n",
    "        self.s3 = boto3.client('s3')\n",
    "        \n",
    "        # Initialize embedding model\n",
    "        self.model_name = model_name\n",
    "        self.vector_dim = vector_dim\n",
    "        print(f\"Loading sentence transformer model: {model_name}\")\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        \n",
    "        # Initialize FAISS index\n",
    "        self.index = None\n",
    "        self.log_data = []\n",
    "        \n",
    "        # Load existing model if path provided\n",
    "        if s3_model_path:\n",
    "            self.load_from_s3(s3_model_path)\n",
    "        else:\n",
    "            # Create new index\n",
    "            self.index = faiss.IndexFlatL2(vector_dim)\n",
    "    \n",
    "    def parse_logs_from_s3(self, s3_path: str, max_logs: Optional[int] = None) -> Tuple[List[Dict[str, Any]], pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Parse logs from S3 location (supports JSON, GZIP)\n",
    "        \n",
    "        Args:\n",
    "            s3_path: S3 URI to the logs\n",
    "            max_logs: Maximum number of logs to process\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (parsed logs, features DataFrame)\n",
    "        \"\"\"\n",
    "        parsed_logs = []\n",
    "        \n",
    "        # Parse S3 URI\n",
    "        parsed_uri = urlparse(s3_path)\n",
    "        bucket = parsed_uri.netloc\n",
    "        key = parsed_uri.path.lstrip('/')\n",
    "        \n",
    "        print(f\"Searching for logs in s3://{bucket}/{key}\")\n",
    "        \n",
    "        # Handle directory with nested structure (year/month/day/hour/minute)\n",
    "        if not key.endswith('.json') and not key.endswith('.gz'):\n",
    "            # List all objects recursively in the directory\n",
    "            paginator = self.s3.get_paginator('list_objects_v2')\n",
    "            files = []\n",
    "            \n",
    "            # Iterate through all pages of results\n",
    "            for page in paginator.paginate(Bucket=bucket, Prefix=key):\n",
    "                if 'Contents' in page:\n",
    "                    for obj in page['Contents']:\n",
    "                        obj_key = obj['Key']\n",
    "                        # Only process .json or .gz files\n",
    "                        if obj_key.endswith('.json') or obj_key.endswith('.gz') or '.json.gz' in obj_key:\n",
    "                            files.append(obj_key)\n",
    "            \n",
    "            print(f\"Found {len(files)} log files\")\n",
    "        else:\n",
    "            # Single file\n",
    "            files = [key]\n",
    "            \n",
    "        # Process each file\n",
    "        for file_key in files:\n",
    "            if not (file_key.endswith('.json') or file_key.endswith('.gz')):\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                # Get the object from S3\n",
    "                print(f\"Processing file: s3://{bucket}/{file_key}\")\n",
    "                obj = self.s3.get_object(Bucket=bucket, Key=file_key)\n",
    "                \n",
    "                if file_key.endswith('.gz') or '.json.gz' in file_key:\n",
    "                    # Decompress and decode\n",
    "                    with gzip.GzipFile(fileobj=io.BytesIO(obj['Body'].read())) as f:\n",
    "                        content = f.read().decode('utf-8')\n",
    "                else:\n",
    "                    # Just decode\n",
    "                    content = obj['Body'].read().decode('utf-8')\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_key}: {e}\")\n",
    "                continue\n",
    "                \n",
    "            # Process each line as a JSON record\n",
    "            for line in content.splitlines():\n",
    "                try:\n",
    "                    log_entry = json.loads(line.strip())\n",
    "                    parsed_logs.append(log_entry)\n",
    "                    \n",
    "                    # Check if we've reached the maximum number of logs\n",
    "                    if max_logs is not None and len(parsed_logs) >= max_logs:\n",
    "                        print(f\"Reached maximum number of logs to process: {max_logs}\")\n",
    "                        break\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Failed to parse line: {line[:100]}...\")\n",
    "                    continue\n",
    "            \n",
    "            # Check if we've reached the maximum number of logs\n",
    "            if max_logs is not None and len(parsed_logs) >= max_logs:\n",
    "                break\n",
    "                    \n",
    "        print(f\"Parsed {len(parsed_logs)} logs\")\n",
    "        \n",
    "        # Extract features\n",
    "        features_df = self.extract_features(parsed_logs)\n",
    "        \n",
    "        return parsed_logs, features_df\n",
    "    \n",
    "    def extract_features(self, logs: List[Dict[str, Any]]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extract relevant features from parsed logs\n",
    "        \n",
    "        Args:\n",
    "            logs: List of parsed log entries\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with extracted features\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        \n",
    "        for log in logs:\n",
    "            # Extract basic log information\n",
    "            log_entry = {\n",
    "                'timestamp': log.get('date'),\n",
    "                'message': log.get('log'),\n",
    "                'log_level': None,\n",
    "                'component': None,\n",
    "                'action': None,\n",
    "                'status': None,\n",
    "                'error_type': None,\n",
    "                'error_details': None,\n",
    "                'pod_name': None,\n",
    "                'namespace': None,\n",
    "                'host': None,\n",
    "                'container_name': None,\n",
    "                'container_image': None\n",
    "            }\n",
    "            \n",
    "            # Extract log level, component and action from message\n",
    "            if log.get('log'):\n",
    "                log_msg = log.get('log')\n",
    "                \n",
    "                # Extract log level\n",
    "                level_match = re.search(r'\\[(info|warn|error|debug)\\]', log_msg, re.IGNORECASE)\n",
    "                if level_match:\n",
    "                    log_entry['log_level'] = level_match.group(1).lower()\n",
    "                \n",
    "                # Extract component\n",
    "                component_match = re.search(r'\\[([^:]+):([^:]+):([^\\]]+)\\]', log_msg)\n",
    "                if component_match:\n",
    "                    log_entry['component'] = f\"{component_match.group(1)}:{component_match.group(2)}:{component_match.group(3)}\"\n",
    "                \n",
    "                # Extract action and status\n",
    "                if \"Successfully\" in log_msg and \"uploaded\" in log_msg:\n",
    "                    log_entry['action'] = \"upload\"\n",
    "                    log_entry['status'] = \"success\"\n",
    "                elif \"Failed to upload\" in log_msg:\n",
    "                    log_entry['action'] = \"upload\"\n",
    "                    log_entry['status'] = \"failure\"\n",
    "                    \n",
    "                    # Extract error details\n",
    "                    error_match = re.search(r'Failed to upload.*: (\\w+)', log_msg)\n",
    "                    if error_match:\n",
    "                        log_entry['error_type'] = error_match.group(1)\n",
    "                        log_entry['error_details'] = log_msg.split(\"Failed to upload\")[1].strip()\n",
    "                elif \"Connection timeout\" in log_msg:\n",
    "                    log_entry['action'] = \"connect\"\n",
    "                    log_entry['status'] = \"failure\"\n",
    "                    log_entry['error_type'] = \"Timeout\"\n",
    "                    \n",
    "                    # Extract timeout details\n",
    "                    timeout_match = re.search(r'timeout after (\\d+)s', log_msg)\n",
    "                    if timeout_match:\n",
    "                        log_entry['error_details'] = f\"Timeout after {timeout_match.group(1)} seconds\"\n",
    "                elif \"Invalid JSON\" in log_msg:\n",
    "                    log_entry['action'] = \"parse\"\n",
    "                    log_entry['status'] = \"failure\"\n",
    "                    log_entry['error_type'] = \"InvalidFormat\"\n",
    "                    log_entry['error_details'] = \"Invalid JSON format\"\n",
    "                elif \"Scanning log file\" in log_msg:\n",
    "                    log_entry['action'] = \"scan\"\n",
    "                    log_entry['status'] = \"info\"\n",
    "                elif \"File rotated\" in log_msg:\n",
    "                    log_entry['action'] = \"rotate\"\n",
    "                    log_entry['status'] = \"info\"\n",
    "                elif \"Retrying\" in log_msg and \"upload\" in log_msg:\n",
    "                    log_entry['action'] = \"retry\"\n",
    "                    log_entry['status'] = \"warning\"\n",
    "                    \n",
    "                    # Extract retry details\n",
    "                    retry_match = re.search(r'attempt (\\d+)/(\\d+)', log_msg)\n",
    "                    if retry_match:\n",
    "                        log_entry['error_details'] = f\"Attempt {retry_match.group(1)} of {retry_match.group(2)}\"\n",
    "                elif \"Started\" in log_msg:\n",
    "                    log_entry['action'] = \"start\"\n",
    "                    log_entry['status'] = \"info\"\n",
    "                elif \"Shutting down\" in log_msg:\n",
    "                    log_entry['action'] = \"shutdown\"\n",
    "                    log_entry['status'] = \"info\"\n",
    "            \n",
    "            # Extract Kubernetes metadata if available\n",
    "            if 'kubernetes' in log:\n",
    "                k8s = log.get('kubernetes', {})\n",
    "                log_entry['pod_name'] = k8s.get('pod_name')\n",
    "                log_entry['namespace'] = k8s.get('namespace_name')\n",
    "                log_entry['host'] = k8s.get('host')\n",
    "                log_entry['container_name'] = k8s.get('container_name')\n",
    "                log_entry['container_image'] = k8s.get('container_image')\n",
    "            \n",
    "            features.append(log_entry)\n",
    "            \n",
    "        return pd.DataFrame(features)\n",
    "    \n",
    "    def generate_embeddings(self, df: pd.DataFrame, text_column: str = 'message') -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for log messages\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame containing logs\n",
    "            text_column: Column containing text to embed\n",
    "            \n",
    "        Returns:\n",
    "            NumPy array of embeddings\n",
    "        \"\"\"\n",
    "        texts = df[text_column].fillna('').tolist()\n",
    "        \n",
    "        # Process in batches to avoid memory issues\n",
    "        batch_size = 128\n",
    "        all_embeddings = []\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} logs in batches of {batch_size}\")\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            batch_embeddings = self.model.encode(batch_texts)\n",
    "            all_embeddings.append(batch_embeddings)\n",
    "            \n",
    "            # Print progress\n",
    "            if (i + batch_size) % 500 == 0 or i + batch_size >= len(texts):\n",
    "                print(f\"Processed {min(i + batch_size, len(texts))}/{len(texts)} logs\")\n",
    "            \n",
    "        # Combine batches\n",
    "        embeddings = np.vstack(all_embeddings)\n",
    "        return embeddings\n",
    "    \n",
    "    def add_to_index(self, df: pd.DataFrame, embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add logs and their embeddings to the index\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame containing logs\n",
    "            embeddings: NumPy array of embeddings\n",
    "        \"\"\"\n",
    "        if len(df) != embeddings.shape[0]:\n",
    "            raise ValueError(\"Number of logs and embeddings must match\")\n",
    "            \n",
    "        # Initialize index if not already done\n",
    "        if self.index is None:\n",
    "            self.index = faiss.IndexFlatL2(self.vector_dim)\n",
    "            \n",
    "        # Store original data\n",
    "        start_idx = len(self.log_data)\n",
    "        self.log_data.extend(df.to_dict('records'))\n",
    "        \n",
    "        # Add embeddings to FAISS index\n",
    "        self.index.add(embeddings.astype(np.float32))\n",
    "        \n",
    "        print(f\"Added {len(df)} logs to index. Total logs: {len(self.log_data)}\")\n",
    "    \n",
    "    def identify_critical_errors(self, df: pd.DataFrame) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Identify critical errors in logs\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame containing logs\n",
    "            \n",
    "        Returns:\n",
    "            List of critical errors\n",
    "        \"\"\"\n",
    "        # Define critical error patterns\n",
    "        critical_patterns = [\n",
    "            # Access denied errors\n",
    "            {'pattern': 'AccessDenied', 'severity': 'high', 'category': 'permission', \n",
    "             'description': 'S3 access denied error', 'column': 'error_type'},\n",
    "            # Connection timeouts\n",
    "            {'pattern': 'Timeout', 'severity': 'medium', 'category': 'network', \n",
    "             'description': 'Connection timeout', 'column': 'error_type'},\n",
    "            # Invalid formats\n",
    "            {'pattern': 'InvalidFormat', 'severity': 'medium', 'category': 'data', \n",
    "             'description': 'Invalid data format', 'column': 'error_type'},\n",
    "            # Retry attempts\n",
    "            {'pattern': 'retry', 'severity': 'low', 'category': 'operation', \n",
    "             'description': 'Operation retry', 'column': 'action'}\n",
    "        ]\n",
    "        \n",
    "        # Find matches\n",
    "        critical_errors = []\n",
    "        \n",
    "        for pattern in critical_patterns:\n",
    "            column = pattern['column']\n",
    "            matches = df[df[column] == pattern['pattern']]\n",
    "            \n",
    "            if len(matches) > 0:\n",
    "                for _, row in matches.iterrows():\n",
    "                    error = {\n",
    "                        'timestamp': row.get('timestamp'),\n",
    "                        'message': row.get('message'),\n",
    "                        'pod_name': row.get('pod_name'),\n",
    "                        'namespace': row.get('namespace'),\n",
    "                        'severity': pattern['severity'],\n",
    "                        'category': pattern['category'],\n",
    "                        'description': pattern['description'],\n",
    "                        'details': row.get('error_details'),\n",
    "                        'count': 1\n",
    "                    }\n",
    "                    critical_errors.append(error)\n",
    "        \n",
    "        # Group similar errors\n",
    "        grouped_errors = {}\n",
    "        for error in critical_errors:\n",
    "            key = f\"{error['severity']}_{error['category']}_{error['description']}\"\n",
    "            if key in grouped_errors:\n",
    "                grouped_errors[key]['count'] += 1\n",
    "                # Keep the most recent occurrence\n",
    "                if error['timestamp'] > grouped_errors[key]['timestamp']:\n",
    "                    grouped_errors[key]['timestamp'] = error['timestamp']\n",
    "                    grouped_errors[key]['message'] = error['message']\n",
    "                    grouped_errors[key]['details'] = error['details']\n",
    "            else:\n",
    "                grouped_errors[key] = error\n",
    "        \n",
    "        return list(grouped_errors.values())\n",
    "    \n",
    "    def identify_patterns(self, df: pd.DataFrame, min_group_size: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Identify patterns in logs based on common attributes\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame containing logs\n",
    "            min_group_size: Minimum number of logs to consider a pattern\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of identified patterns\n",
    "        \"\"\"\n",
    "        patterns = {}\n",
    "        \n",
    "        # Group by component and action\n",
    "        for (component, action), group_df in df.groupby(['component', 'action']):\n",
    "            if pd.isna(component) or pd.isna(action) or len(group_df) < min_group_size:\n",
    "                continue\n",
    "                \n",
    "            # Get the most common status\n",
    "            status = group_df['status'].mode().iloc[0] if not group_df['status'].isna().all() else 'unknown'\n",
    "            \n",
    "            # Calculate time intervals\n",
    "            if 'timestamp' in group_df.columns and len(group_df) > 1:\n",
    "                try:\n",
    "                    # Convert to datetime if it's not already\n",
    "                    if not pd.api.types.is_datetime64_any_dtype(group_df['timestamp']):\n",
    "                        group_df['timestamp'] = pd.to_datetime(group_df['timestamp'])\n",
    "                        \n",
    "                    # Sort and calculate intervals\n",
    "                    sorted_df = group_df.sort_values('timestamp')\n",
    "                    sorted_df['time_diff'] = sorted_df['timestamp'].diff().dt.total_seconds()\n",
    "                    \n",
    "                    median_interval = sorted_df['time_diff'].median()\n",
    "                    mean_interval = sorted_df['time_diff'].mean()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error calculating time intervals: {e}\")\n",
    "                    median_interval = None\n",
    "                    mean_interval = None\n",
    "            else:\n",
    "                median_interval = None\n",
    "                mean_interval = None\n",
    "            \n",
    "            # Create pattern entry\n",
    "            pattern_key = f\"{component}_{action}\"\n",
    "            patterns[pattern_key] = {\n",
    "                'component': component,\n",
    "                'action': action,\n",
    "                'status': status,\n",
    "                'count': len(group_df),\n",
    "                'median_interval_seconds': float(median_interval) if pd.notna(median_interval) else None,\n",
    "                'mean_interval_seconds': float(mean_interval) if pd.notna(mean_interval) else None,\n",
    "                'sample_logs': group_df.head(3)['message'].tolist()\n",
    "            }\n",
    "        \n",
    "        return patterns\n",
    "    \n",
    "    def generate_suggested_fixes(self, critical_errors: List[Dict[str, Any]]) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Generate suggested fixes for critical errors\n",
    "        \n",
    "        Args:\n",
    "            critical_errors: List of critical errors\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of suggested fixes\n",
    "        \"\"\"\n",
    "        # Common fixes for known error types\n",
    "        common_fixes = {\n",
    "            'AccessDenied': [\n",
    "                \"Check IAM permissions for the FluentBit service account\",\n",
    "                \"Verify S3 bucket policy allows write access from the cluster's IP range\",\n",
    "                \"Ensure KMS key permissions are properly configured if using SSE-KMS\"\n",
    "            ],\n",
    "            'Timeout': [\n",
    "                \"Check network connectivity between the cluster and S3 endpoint\",\n",
    "                \"Verify VPC endpoints are properly configured\",\n",
    "                \"Consider increasing the timeout setting in FluentBit configuration\"\n",
    "            ],\n",
    "            'InvalidFormat': [\n",
    "                \"Review the log format configuration in FluentBit\",\n",
    "                \"Check for malformed JSON in application logs\",\n",
    "                \"Add a parser filter to handle the specific log format\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        suggested_fixes = {}\n",
    "        \n",
    "        for error in critical_errors:\n",
    "            if error['severity'] == 'low':\n",
    "                continue\n",
    "                \n",
    "            key = f\"{error['severity']}_{error['category']}_{error['description']}\"\n",
    "            \n",
    "            # Get fixes based on error type\n",
    "            if error['error_type'] in common_fixes:\n",
    "                suggested_fixes[key]
