{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Summarization and Analysis Notebook\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load processed logs from Glue/S3\n",
    "2. Generate human-readable summaries\n",
    "3. Identify critical errors\n",
    "4. Suggest potential fixes\n",
    "\n",
    "It builds on the existing log processing infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# For embedding generation\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# For vector storage\n",
    "import faiss\n",
    "\n",
    "# For clustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# For LLM-based summarization\n",
    "import openai\n",
    "\n",
    "# Configure AWS services\n",
    "s3 = boto3.client('s3')\n",
    "athena = boto3.client('athena')\n",
    "glue = boto3.client('glue')\n",
    "\n",
    "# Set your OpenAI API key\n",
    "# openai.api_key = \"your-api-key\"  # Uncomment and set your API key\n",
    "\n",
    "# S3 bucket and paths\n",
    "LOG_BUCKET = \"first-order-application-logs\"\n",
    "ATHENA_RESULTS = f\"s3://{LOG_BUCKET}/athena-results/\"\n",
    "MODEL_PATH = f\"s3://{LOG_BUCKET}/models/\"\n",
    "\n",
    "# Glue database and table\n",
    "GLUE_DATABASE = \"first-order-glue-db\"\n",
    "LOGS_TABLE = \"fluent_bit_logs\"  # This should match the table created by your Glue crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Process Log Data\n",
    "\n",
    "First, we'll query Athena to get the processed logs from the Glue database, then extract features and generate embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def run_athena_query(query, database=GLUE_DATABASE, output_location=ATHENA_RESULTS):\n",
    "    \"\"\"Run a query on Athena and return the results as a DataFrame\"\"\"\n",
    "    response = athena.start_query_execution(\n",
    "        QueryString=query,\n",
    "        QueryExecutionContext={\n",
    "            'Database': database\n",
    "        },\n",
    "        ResultConfiguration={\n",
    "            'OutputLocation': output_location,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    query_execution_id = response['QueryExecutionId']\n",
    "    \n",
    "    # Wait for query to complete\n",
    "    state = 'RUNNING'\n",
    "    while state in ['RUNNING', 'QUEUED']:\n",
    "        response = athena.get_query_execution(QueryExecutionId=query_execution_id)\n",
    "        state = response['QueryExecution']['Status']['State']\n",
    "        if state == 'FAILED':\n",
    "            raise Exception(f\"Query failed: {response['QueryExecution']['Status']['StateChangeReason']}\")\n",
    "        elif state == 'SUCCEEDED':\n",
    "            # Get the results\n",
    "            results = athena.get_query_results(QueryExecutionId=query_execution_id)\n",
    "            \n",
    "            # Parse the results into a DataFrame\n",
    "            columns = [col['Label'] for col in results['ResultSet']['ResultSetMetadata']['ColumnInfo']]\n",
    "            rows = []\n",
    "            for row in results['ResultSet']['Rows'][1:]:  # Skip the header row\n",
    "                data = [field.get('VarCharValue', '') if 'VarCharValue' in field else None for field in row['Data']]\n",
    "                rows.append(data)\n",
    "            \n",
    "            df = pd.DataFrame(rows, columns=columns)\n",
    "            return df\n",
    "    \n",
    "    return None\n",
    "\n",
    "def extract_features(logs_df):\n",
    "    \"\"\"Extract features from logs DataFrame\"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df = logs_df.copy()\n",
    "    \n",
    "    # Initialize new columns\n",
    "    df['log_level'] = None\n",
    "    df['component'] = None\n",
    "    df['action'] = None\n",
    "    df['status'] = None\n",
    "    df['error_type'] = None\n",
    "    df['error_details'] = None\n",
    "    \n",
    "    # Extract features from log messages\n",
    "    for idx, row in df.iterrows():\n",
    "        log_msg = row.get('log', '')\n",
    "        \n",
    "        # Extract log level\n",
    "        level_match = re.search(r'\\[(info|warn|error|debug)\\]', log_msg, re.IGNORECASE)\n",
    "        if level_match:\n",
    "            df.at[idx, 'log_level'] = level_match.group(1).lower()\n",
    "        \n",
    "        # Extract component\n",
    "        component_match = re.search(r'\\[([^:]+):([^:]+):([^\\]]+)\\]', log_msg)\n",
    "        if component_match:\n",
    "            df.at[idx, 'component'] = f\"{component_match.group(1)}:{component_match.group(2)}:{component_match.group(3)}\"\n",
    "        \n",
    "        # Extract action and status\n",
    "        if \"Successfully\" in log_msg and \"uploaded\" in log_msg:\n",
    "            df.at[idx, 'action'] = \"upload\"\n",
    "            df.at[idx, 'status'] = \"success\"\n",
    "        elif \"Failed to upload\" in log_msg:\n",
    "            df.at[idx, 'action'] = \"upload\"\n",
    "            df.at[idx, 'status'] = \"failure\"\n",
    "            \n",
    "            # Extract error details\n",
    "            error_match = re.search(r'Failed to upload.*: (\\w+)', log_msg)\n",
    "            if error_match:\n",
    "                df.at[idx, 'error_type'] = error_match.group(1)\n",
    "                df.at[idx, 'error_details'] = log_msg.split(\"Failed to upload\")[1].strip()\n",
    "        elif \"Connection timeout\" in log_msg:\n",
    "            df.at[idx, 'action'] = \"connect\"\n",
    "            df.at[idx, 'status'] = \"failure\"\n",
    "            df.at[idx, 'error_type'] = \"Timeout\"\n",
    "            \n",
    "            # Extract timeout details\n",
    "            timeout_match = re.search(r'timeout after (\\d+)s', log_msg)\n",
    "            if timeout_match:\n",
    "                df.at[idx, 'error_details'] = f\"Timeout after {timeout_match.group(1)} seconds\"\n",
    "        elif \"Invalid JSON\" in log_msg:\n",
    "            df.at[idx, 'action'] = \"parse\"\n",
    "            df.at[idx, 'status'] = \"failure\"\n",
    "            df.at[idx, 'error_type'] = \"InvalidFormat\"\n",
    "            df.at[idx, 'error_details'] = \"Invalid JSON format\"\n",
    "        elif \"Scanning log file\" in log_msg:\n",
    "            df.at[idx, 'action'] = \"scan\"\n",
    "            df.at[idx, 'status'] = \"info\"\n",
    "        elif \"File rotated\" in log_msg:\n",
    "            df.at[idx, 'action'] = \"rotate\"\n",
    "            df.at[idx, 'status'] = \"info\"\n",
    "        elif \"Retrying\" in log_msg and \"upload\" in log_msg:\n",
    "            df.at[idx, 'action'] = \"retry\"\n",
    "            df.at[idx, 'status'] = \"warning\"\n",
    "            \n",
    "            # Extract retry details\n",
    "            retry_match = re.search(r'attempt (\\d+)/(\\d+)', log_msg)\n",
    "            if retry_match:\n",
    "                df.at[idx, 'error_details'] = f\"Attempt {retry_match.group(1)} of {retry_match.group(2)}\"\n",
    "        elif \"Started\" in log_msg:\n",
    "            df.at[idx, 'action'] = \"start\"\n",
    "            df.at[idx, 'status'] = \"info\"\n",
    "        elif \"Shutting down\" in log_msg:\n",
    "            df.at[idx, 'action'] = \"shutdown\"\n",
    "            df.at[idx, 'status'] = \"info\"\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Query logs from the last 24 hours\n",
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM {LOGS_TABLE}\n",
    "WHERE date >= TIMESTAMP '{(datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d %H:%M:%S')}'\n",
    "LIMIT 1000\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    logs_df = run_athena_query(query)\n",
    "    print(f\"Retrieved {len(logs_df)} logs\")\n",
    "except Exception as e:\n",
    "    print(f\"Error querying logs: {e}\")\n",
    "    print(\"Using sample data instead...\")\n",
    "    \n",
    "    # Create sample data for demonstration\n",
    "    logs_df = pd.DataFrame({\n",
    "        'date': [datetime.now() - timedelta(minutes=i) for i in range(10)],\n",
    "        'log': [\n",
    "            \"[2025/04/01 23:59:58] [ info] [output:s3:s3.0] Successfully uploaded object /path/to/object\",\n",
    "            \"[2025/04/01 23:59:57] [ info] [input:tail:tail.0] Scanning log file /var/log/containers/app-xyz.log\",\n",
    "            \"[2025/04/01 23:59:56] [ error] [output:s3:s3.0] Failed to upload object: AccessDenied\",\n",
    "            \"[2025/04/01 23:59:55] [ warn] [filter:kubernetes:kubernetes.0] Missing annotation\",\n",
    "            \"[2025/04/01 23:59:54] [ error] [output:s3:s3.0] Connection timeout after 30s\",\n",
    "            \"[2025/04/01 23:59:53] [ info] [input:tail:tail.0] File rotated: /var/log/containers/app-xyz.log\",\n",
    "            \"[2025/04/01 23:59:52] [ error] [parser:json:json.0] Invalid JSON format\",\n",
    "            \"[2025/04/01 23:59:51] [ warn] [output:s3:s3.0] Retrying upload (attempt 3/5)\",\n",
    "            \"[2025/04/01 23:59:50] [ info] [engine] Shutting down\",\n",
    "            \"[2025/04/01 23:59:49] [ info] [engine] Started (version 1.8.15)\"\n",
    "        ],\n",
    "        'kubernetes.pod_name': ['app-xyz-1234', 'app-xyz-1234', 'app-xyz-1234', 'app-xyz-1234', 'app-xyz-1234',\n",
    "                              'app-xyz-1234', 'app-xyz-1234', 'app-xyz-1234', 'fluent-bit-qmn54', 'fluent-bit-qmn54'],\n",
    "        'kubernetes.namespace_name': ['default', 'default', 'default', 'default', 'default',\n",
    "                                    'default', 'default', 'default', 'logging', 'logging']\n",
    "    })\n",
    "\n",
    "# Extract features\n",
    "features_df = extract_features(logs_df)\n",
    "display(features_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Identify Critical Errors and Patterns\n",
    "\n",
    "Now we'll identify critical errors and patterns in the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def identify_critical_errors(df):\n",
    "    \"\"\"Identify critical errors in logs\"\"\"\n",
    "    # Define critical error patterns\n",
    "    critical_patterns = [\n",
    "        # Access denied errors\n",
    "        {'pattern': 'AccessDenied', 'severity': 'high', 'category': 'permission', \n",
    "         'description': 'S3 access denied error', 'column': 'error_type'},\n",
    "        # Connection timeouts\n",
    "        {'pattern': 'Timeout', 'severity': 'medium', 'category': 'network', \n",
    "         'description': 'Connection timeout', 'column': 'error_type'},\n",
    "        # Invalid formats\n",
    "        {'pattern': 'InvalidFormat', 'severity': 'medium', 'category': 'data', \n",
    "         'description': 'Invalid data format', 'column': 'error_type'},\n",
    "        # Retry attempts\n",
    "        {'pattern': 'retry', 'severity': 'low', 'category': 'operation', \n",
    "         'description': 'Operation retry', 'column': 'action'}\n",
    "    ]\n",
    "    \n",
    "    # Find matches\n",
    "    critical_errors = []\n",
    "    \n",
    "    for pattern in critical_patterns:\n",
    "        column = pattern['column']\n",
    "        matches = df[df[column] == pattern['pattern']]\n",
    "        \n",
    "        if len(matches) > 0:\n",
    "            for _, row in matches.iterrows():\n",
    "                error = {\n",
    "                    'timestamp': row.get('date'),\n",
    "                    'log': row.get('log'),\n",
    "                    'pod_name': row.get('kubernetes.pod_name'),\n",
    "                    'namespace': row.get('kubernetes.namespace_name'),\n",
    "                    'severity': pattern['severity'],\n",
    "                    'category': pattern['category'],\n",
    "                    'description': pattern['description'],\n",
    "                    'details': row.get('error_details'),\n",
    "                    'count': 1\n",
    "                }\n",
    "                critical_errors.append(error)\n",
    "    \n",
    "    # Group similar errors\n",
    "    grouped_errors = {}\n",
    "    for error in critical_errors:\n",
    "        key = f\"{error['severity']}_{error['category']}_{error['description']}\"\n",
    "        if key in grouped_errors:\n",
    "            grouped_errors[key]['count'] += 1\n",
    "            # Keep the most recent occurrence\n",
    "            if error['timestamp'] > grouped_errors[key]['timestamp']:\n",
    "                grouped_errors[key]['timestamp'] = error['timestamp']\n",
    "                grouped_errors[key]['log'] = error['log']\n",
    "                grouped_errors[key]['details'] = error['details']\n",
    "        else:\n",
    "            grouped_errors[key] = error\n",
    "    \n",
    "    return list(grouped_errors.values())\n",
    "\n",
    "def identify_patterns(df):\n",
    "    \"\"\"Identify patterns in logs\"\"\"\n",
    "    patterns = {}\n",
    "    \n",
    "    # Group by component and action\n",
    "    for (component, action), group_df in df.groupby(['component', 'action']):\n",
    "        if pd.isna(component) or pd.isna(action) or len(group_df) < 2:\n",
    "            continue\n",
    "            \n",
    "        # Get the most common status\n",
    "        status = group_df['status'].mode().iloc[0] if not group_df['status'].isna().all() else 'unknown'\n",
    "        \n",
    "        # Create pattern entry\n",
    "        pattern_key = f\"{component}_{action}\"\n",
    "        patterns[pattern_key] = {\n",
    "            'component': component,\n",
    "            'action': action,\n",
    "            'status': status,\n",
    "            'count': len(group_df),\n",
    "            'sample_logs': group_df.head(3)['log'].tolist()\n",
    "        }\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "# Identify critical errors\n",
    "critical_errors = identify_critical_errors(features_df)\n",
    "print(f\"Identified {len(critical_errors)} critical errors\")\n",
    "for error in critical_errors:\n",
    "    print(f\"- {error['severity'].upper()} {error['category']}: {error['description']} (count: {error['count']})\")\n",
    "    print(f\"  Example: {error['log']}\")\n",
    "    print()\n",
    "\n",
    "# Identify patterns\n",
    "patterns = identify_patterns(features_df)\n",
    "print(f\"\\nIdentified {len(patterns)} patterns\")\n",
    "for key, pattern in list(patterns.items())[:3]:  # Show first 3 patterns\n",
    "    print(f\"- {pattern['component']} {pattern['action']} ({pattern['count']} logs)\")\n",
    "    print(f\"  Example: {pattern['sample_logs'][0] if pattern['sample_logs'] else 'No sample'}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Human-Readable Summaries and Suggest Fixes\n",
    "\n",
    "Now we'll generate human-readable summaries and suggest fixes for the critical errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example fixes for common errors\n",
    "common_fixes = {\n",
    "    'AccessDenied': [\n",
    "        \"Check IAM permissions for the FluentBit service account\",\n",
    "        \"Verify S3 bucket policy allows write access from the cluster's IP range\",\n",
    "        \"Ensure KMS key permissions are properly configured if using SSE-KMS\"\n",
    "    ],\n",
    "    'Timeout': [\n",
    "        \"Check network connectivity between the cluster and S3 endpoint\",\n",
    "        \"Verify VPC endpoints are properly configured\",\n",
    "        \"Consider increasing the timeout setting in FluentBit configuration\"\n",
    "    ],\n",
    "    'InvalidFormat': [\n",
    "        \"Review the log format configuration in FluentBit\",\n",
    "        \"Check for malformed JSON in application logs\",\n",
    "        \"Add a parser filter to handle the specific log format\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Display suggested fixes for identified errors\n",
    "print(\"Suggested Fixes for Critical Errors:\\n\")\n",
    "for error in critical_errors:\n",
    "    if error['severity'] == 'low':\n",
    "        continue\n",
    "        \n",
    "    print(f\"For {error['severity'].upper()} {error['category']}: {error['description']}\")\n",
    "    \n",
    "    # Get fixes based on error type\n",
    "    if error['error_type'] in common_fixes:\n",
    "        for i, fix in enumerate(common_fixes[error['error_type']], 1):\n",
    "            print(f\"  {i}. {fix}\")\n",
    "    else:\n",
    "        print(\"  No specific fixes available for this error type.\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Next Steps for Log Processing Pipeline\n",
    "\n",
    "Based on the analysis, here are the recommended next steps to enhance the log processing pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display next steps for the log processing pipeline\n",
    "next_steps = [\n",
    "    \"1. **Deploy the Glue Job**: Configure and deploy the log_processor.py as a scheduled Glue job\",\n",
    "    \"2. **Set Up Monitoring**: Create CloudWatch alarms for critical error patterns\",\n",
    "    \"3. **Implement Automated Remediation**: Use AWS Lambda to automatically fix common issues\",\n",
    "    \"4. **Create Dashboard**: Build a QuickSight dashboard for log insights\",\n",
    "    \"5. **Integrate with ChatOps**: Send summaries to Slack/Teams channels\"\n",
    "]\n",
    "\n",
    "print(\"# Recommended Next Steps for Log Processing Pipeline\\n\")\n",
    "for step in next_steps:\n",
    "    print(step)\n",
    "    print()\n",
    "\n",
    "print(\"## Implementation Plan\\n\")\n",
    "print(\"1. **Short-term (1-2 weeks)**\")\n",
    "print(\"   - Configure and deploy the Glue job with daily schedule\")\n",
    "print(\"   - Set up basic CloudWatch alarms for critical errors\")\n",
    "print()\n",
    "print(\"2. **Medium-term (2-4 weeks)**\")\n",
    "print(\"   - Develop QuickSight dashboard for log insights\")\n",
    "print(\"   - Implement ChatOps integration for daily summaries\")\n",
    "print()\n",
    "print(\"3. **Long-term (1-2 months)**\")\n",
    "print(\"   - Implement automated remediation with AWS Lambda\")\n",
    "print(\"   - Enhance the log processor with more advanced ML capabilities\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
