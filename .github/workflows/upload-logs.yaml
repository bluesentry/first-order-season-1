name: Upload Workflow Logs to S3 for LLM Analysis

on:
  # Trigger after the Terraform workflow completes
  workflow_run:
    workflows: ["Terraform Deploy AWS Infrastructure"]
    types:
      - completed
  # Allow manual triggering
  workflow_dispatch:
    inputs:
      run_id:
        description: 'Run ID of the Terraform workflow to download logs from (leave empty for latest)'
        required: false
      workflow_name:
        description: 'Name of the workflow to download logs from (default: Terraform Deploy AWS Infrastructure)'
        required: false
        default: 'Terraform Deploy AWS Infrastructure'

permissions:
  id-token: write  # Required for OIDC authentication with AWS
  contents: read
  actions: read    # Required to download workflow logs

env:
  AWS_REGION: us-east-1
  AWS_ACCOUNT_ID: 704855531002
  ROLE_NAME: BlueSentry
  S3_BUCKET: first-order-application-logs
  S3_PREFIX: log-analysis

jobs:
  upload-logs-for-llm:
    runs-on: ubuntu-latest
    
    steps:
      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: arn:aws:iam::${{ env.AWS_ACCOUNT_ID }}:role/${{ env.ROLE_NAME }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Generate timestamp
        id: timestamp
        run: echo "TIMESTAMP=$(date +'%Y-%m-%d-%H-%M-%S')" >> $GITHUB_ENV
      
      - name: Create temp directory for logs
        run: mkdir -p /tmp/workflow-logs
      
      - name: Set workflow name
        run: |
          if [ "${{ github.event_name }}" == "workflow_dispatch" ] && [ -n "${{ github.event.inputs.workflow_name }}" ]; then
            echo "WORKFLOW_NAME=${{ github.event.inputs.workflow_name }}" >> $GITHUB_ENV
          else
            echo "WORKFLOW_NAME=Terraform Deploy AWS Infrastructure" >> $GITHUB_ENV
          fi
      
      - name: Determine run ID
        id: determine-run-id
        run: |
          if [ "${{ github.event_name }}" == "workflow_dispatch" ] && [ -n "${{ github.event.inputs.run_id }}" ]; then
            echo "RUN_ID=${{ github.event.inputs.run_id }}" >> $GITHUB_ENV
          elif [ "${{ github.event_name }}" == "workflow_run" ]; then
            echo "RUN_ID=${{ github.event.workflow_run.id }}" >> $GITHUB_ENV
          else
            # Get the latest run ID of the specified workflow
            REPO="${{ github.repository }}"
            WORKFLOW_ID=$(curl -s \
              -H "Accept: application/vnd.github+json" \
              -H "Authorization: Bearer ${{ secrets.GITHUB_TOKEN }}" \
              -H "X-GitHub-Api-Version: 2022-11-28" \
              "https://api.github.com/repos/${REPO}/actions/workflows" | \
              jq -r '.workflows[] | select(.name=="${{ env.WORKFLOW_NAME }}") | .id')
            
            LATEST_RUN_ID=$(curl -s \
              -H "Accept: application/vnd.github+json" \
              -H "Authorization: Bearer ${{ secrets.GITHUB_TOKEN }}" \
              -H "X-GitHub-Api-Version: 2022-11-28" \
              "https://api.github.com/repos/${REPO}/actions/workflows/${WORKFLOW_ID}/runs?per_page=1" | \
              jq -r '.workflow_runs[0].id')
            
            echo "RUN_ID=${LATEST_RUN_ID}" >> $GITHUB_ENV
          fi
          
          echo "Using Workflow: ${{ env.WORKFLOW_NAME }}"
          echo "Using Run ID: ${{ env.RUN_ID }}"
      
      - name: Download workflow logs
        run: |
          # Download logs using GitHub API
          echo "Downloading logs for workflow run ${{ env.RUN_ID }}..."
          
          # Get workflow run info
          REPO="${{ github.repository }}"
          API_URL="https://api.github.com/repos/${REPO}/actions/runs/${{ env.RUN_ID }}/logs"
          
          # Download logs zip file
          curl -L -o /tmp/workflow-logs/logs.zip \
            -H "Accept: application/vnd.github+json" \
            -H "Authorization: Bearer ${{ secrets.GITHUB_TOKEN }}" \
            -H "X-GitHub-Api-Version: 2022-11-28" \
            ${API_URL}
          
          # Extract logs
          unzip -o /tmp/workflow-logs/logs.zip -d /tmp/workflow-logs/
          
          # List downloaded logs
          echo "Downloaded logs:"
          ls -la /tmp/workflow-logs/
      
      - name: Get workflow run details
        run: |
          REPO="${{ github.repository }}"
          RUN_DETAILS=$(curl -s \
            -H "Accept: application/vnd.github+json" \
            -H "Authorization: Bearer ${{ secrets.GITHUB_TOKEN }}" \
            -H "X-GitHub-Api-Version: 2022-11-28" \
            "https://api.github.com/repos/${REPO}/actions/runs/${{ env.RUN_ID }}")
          
          # Extract workflow name and other details
          WORKFLOW_ID=$(echo $RUN_DETAILS | jq -r '.workflow_id')
          RUN_NUMBER=$(echo $RUN_DETAILS | jq -r '.run_number')
          RUN_ATTEMPT=$(echo $RUN_DETAILS | jq -r '.run_attempt')
          EVENT_NAME=$(echo $RUN_DETAILS | jq -r '.event')
          ACTOR=$(echo $RUN_DETAILS | jq -r '.actor.login')
          CONCLUSION=$(echo $RUN_DETAILS | jq -r '.conclusion')
          STATUS=$(echo $RUN_DETAILS | jq -r '.status')
          CREATED_AT=$(echo $RUN_DETAILS | jq -r '.created_at')
          UPDATED_AT=$(echo $RUN_DETAILS | jq -r '.updated_at')
          
          # Create metadata file with workflow information
          cat > /tmp/workflow-logs/workflow-info.json << EOF
          {
            "repository": "${{ github.repository }}",
            "workflow_name": "${{ env.WORKFLOW_NAME }}",
            "workflow_id": "${WORKFLOW_ID}",
            "run_id": "${{ env.RUN_ID }}",
            "run_number": "${RUN_NUMBER}",
            "run_attempt": "${RUN_ATTEMPT}",
            "event_name": "${EVENT_NAME}",
            "actor": "${ACTOR}",
            "conclusion": "${CONCLUSION}",
            "status": "${STATUS}",
            "created_at": "${CREATED_AT}",
            "updated_at": "${UPDATED_AT}",
            "timestamp": "${{ env.TIMESTAMP }}"
          }
          EOF
      
      - name: Prepare logs for LLM analysis
        run: |
          # Create a directory for processed logs
          mkdir -p /tmp/workflow-logs/processed
          
          # Create a consolidated log file with all logs
          echo "# Workflow Logs for LLM Analysis" > /tmp/workflow-logs/processed/consolidated_logs.md
          echo "" >> /tmp/workflow-logs/processed/consolidated_logs.md
          echo "## Workflow Information" >> /tmp/workflow-logs/processed/consolidated_logs.md
          echo "" >> /tmp/workflow-logs/processed/consolidated_logs.md
          echo "- **Repository:** ${{ github.repository }}" >> /tmp/workflow-logs/processed/consolidated_logs.md
          echo "- **Workflow:** ${{ env.WORKFLOW_NAME }}" >> /tmp/workflow-logs/processed/consolidated_logs.md
          echo "- **Run ID:** ${{ env.RUN_ID }}" >> /tmp/workflow-logs/processed/consolidated_logs.md
          echo "- **Timestamp:** ${{ env.TIMESTAMP }}" >> /tmp/workflow-logs/processed/consolidated_logs.md
          echo "" >> /tmp/workflow-logs/processed/consolidated_logs.md
          
          # Add each log file to the consolidated file
          echo "## Log Files" >> /tmp/workflow-logs/processed/consolidated_logs.md
          echo "" >> /tmp/workflow-logs/processed/consolidated_logs.md
          
          # Create a directory to store sanitized log files
          mkdir -p /tmp/workflow-logs/sanitized
          
          # First, sanitize log filenames by copying them to a new directory with safe names
          log_index=1
          find /tmp/workflow-logs -type f -name "*.txt" | sort | while read log_file; do
            original_filename=$(basename "$log_file")
            safe_filename="log_${log_index}_${original_filename//[^a-zA-Z0-9._-]/_}"
            
            # Copy the file with a safe name
            cp "$log_file" "/tmp/workflow-logs/sanitized/$safe_filename"
            echo "Sanitized $original_filename to $safe_filename"
            
            ((log_index++))
          done
          
          # Process each sanitized log file
          for log_file in $(find /tmp/workflow-logs/sanitized -type f | sort); do
            filename=$(basename "$log_file")
            echo "Processing $filename..."
            
            # Add log file header
            echo "### $filename" >> /tmp/workflow-logs/processed/consolidated_logs.md
            echo "" >> /tmp/workflow-logs/processed/consolidated_logs.md
            echo '```' >> /tmp/workflow-logs/processed/consolidated_logs.md
            
            # Add log content - use cat with error handling
            if [ -f "$log_file" ]; then
              cat "$log_file" >> /tmp/workflow-logs/processed/consolidated_logs.md 2>/dev/null || echo "Error reading file content" >> /tmp/workflow-logs/processed/consolidated_logs.md
            else
              echo "File not found: $log_file" >> /tmp/workflow-logs/processed/consolidated_logs.md
            fi
            
            # Close code block
            echo '```' >> /tmp/workflow-logs/processed/consolidated_logs.md
            echo "" >> /tmp/workflow-logs/processed/consolidated_logs.md
          done
          
          # Create a JSON version for easier parsing by LLMs
          echo "Creating JSON version of logs..."
          
          # Start JSON structure
          echo "{" > /tmp/workflow-logs/processed/logs.json
          echo "  \"workflow_info\": $(cat /tmp/workflow-logs/workflow-info.json 2>/dev/null || echo '{}')," >> /tmp/workflow-logs/processed/logs.json
          echo "  \"log_files\": [" >> /tmp/workflow-logs/processed/logs.json
          
          # Add each sanitized log file as a JSON object
          first_file=true
          for log_file in $(find /tmp/workflow-logs/sanitized -type f | sort); do
            filename=$(basename "$log_file")
            
            # Add comma for all but the first file
            if [ "$first_file" = true ]; then
              first_file=false
            else
              echo "," >> /tmp/workflow-logs/processed/logs.json
            fi
            
            # Safely read and escape log content
            if [ -f "$log_file" ]; then
              # Use a safer approach to escape JSON content
              log_content=$(cat "$log_file" 2>/dev/null | 
                           tr -d '\000' |  # Remove null bytes
                           sed 's/\\/\\\\/g' | # Escape backslashes
                           sed 's/"/\\"/g' |  # Escape quotes
                           sed ':a;N;$!ba;s/\n/\\n/g') # Replace newlines
              
              # If log_content is empty due to errors, provide a placeholder
              if [ -z "$log_content" ]; then
                log_content="Error reading log content"
              fi
            else
              log_content="File not found"
            fi
            
            # Add log file as JSON object
            echo "    {" >> /tmp/workflow-logs/processed/logs.json
            echo "      \"filename\": \"$filename\"," >> /tmp/workflow-logs/processed/logs.json
            echo "      \"content\": \"$log_content\"" >> /tmp/workflow-logs/processed/logs.json
            echo -n "    }" >> /tmp/workflow-logs/processed/logs.json
          done
          
          # Close JSON structure
          echo "" >> /tmp/workflow-logs/processed/logs.json
          echo "  ]" >> /tmp/workflow-logs/processed/logs.json
          echo "}" >> /tmp/workflow-logs/processed/logs.json
          
          # Create a summary file with key information
          echo "Creating summary file..."
          cat > /tmp/workflow-logs/processed/summary.md << EOF
          # Workflow Log Summary
          
          ## Overview
          
          - **Repository:** ${{ github.repository }}
          - **Workflow:** ${{ env.WORKFLOW_NAME }}
          - **Run ID:** ${{ env.RUN_ID }}
          - **Run Number:** $(cat /tmp/workflow-logs/workflow-info.json | jq -r '.run_number')
          - **Triggered By:** $(cat /tmp/workflow-logs/workflow-info.json | jq -r '.actor')
          - **Event:** $(cat /tmp/workflow-logs/workflow-info.json | jq -r '.event_name')
          - **Status:** $(cat /tmp/workflow-logs/workflow-info.json | jq -r '.status')
          - **Conclusion:** $(cat /tmp/workflow-logs/workflow-info.json | jq -r '.conclusion')
          - **Created At:** $(cat /tmp/workflow-logs/workflow-info.json | jq -r '.created_at')
          - **Updated At:** $(cat /tmp/workflow-logs/workflow-info.json | jq -r '.updated_at')
          
          ## Log Files
          
          $(find /tmp/workflow-logs -type f -name "*.txt" | wc -l) log files were captured from this workflow run.
          
          ## Analysis Instructions
          
          These logs are intended for analysis by a Large Language Model (LLM). The logs contain:
          
          1. Workflow execution details
          2. Terraform operations (plan, apply, or destroy)
          3. Infrastructure changes
          4. Error messages (if any)
          
          When analyzing these logs, focus on:
          
          - Identifying successful vs. failed operations
          - Extracting key infrastructure changes
          - Highlighting any errors or warnings
          - Providing a summary of the Terraform execution
          EOF
      
      - name: Upload logs to S3
        run: |
          # Upload all logs to S3
          echo "Uploading workflow logs to S3..."
          aws s3 sync /tmp/workflow-logs/ s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}/workflow-logs-for-llm/${{ env.TIMESTAMP }}/raw/
          
          # Upload processed logs to a separate prefix
          echo "Uploading processed logs for LLM analysis..."
          aws s3 sync /tmp/workflow-logs/processed/ s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}/workflow-logs-for-llm/${{ env.TIMESTAMP }}/processed/
          
          echo "Logs uploaded successfully to:"
          echo "- Raw logs: s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}/workflow-logs-for-llm/${{ env.TIMESTAMP }}/raw/"
          echo "- Processed logs: s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}/workflow-logs-for-llm/${{ env.TIMESTAMP }}/processed/"
